# Cuttle ğŸ¦€

ä¸€ä¸ªåŸºäºCPUçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ï¼Œä½¿ç”¨çº¯Rustå®ç°ï¼Œä¸“é—¨ä¼˜åŒ–æ”¯æŒQwen3-0.6Bæ¨¡å‹ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¦€ **çº¯Rustå®ç°**: æ— Pythonä¾èµ–ï¼Œé«˜æ€§èƒ½CPUæ¨ç†
- ğŸ¤– **Qwen3-0.6Bæ”¯æŒ**: ä¸“é—¨ä¼˜åŒ–æ”¯æŒQwen3-0.6Bæ¨¡å‹
- ğŸŒ **ä¸­è‹±æ–‡åŒè¯­**: æ”¯æŒä¸­è‹±æ–‡åŒè¯­æ–‡æœ¬ç”Ÿæˆ
- ğŸ“¦ **è‡ªåŠ¨ä¸‹è½½**: è‡ªåŠ¨æ¨¡å‹ä¸‹è½½åŠŸèƒ½
- ğŸ’» **å‘½ä»¤è¡Œç•Œé¢**: æ˜“äºä½¿ç”¨çš„CLIå·¥å…·
- ğŸ”§ **çµæ´»é…ç½®**: å¯é…ç½®çš„æ¨ç†å‚æ•°å’Œåˆ†è¯ç³»ç»Ÿ
- ğŸ“Š **æ€§èƒ½ç›‘æ§**: å†…ç½®æ€§èƒ½åˆ†æå’ŒåŸºå‡†æµ‹è¯•

## ğŸ—ï¸ Architecture

Cuttle adopts a modular design with the following main components:

- **Tensor Module** (`tensor`): High-performance tensor operations using pure Rust
- **Model Module** (`model`): Transformer architecture implementation
- **Tokenizer Module** (`tokenizer`): Text tokenization and encoding
- **Inference Engine** (`inference`): Complete inference pipeline
- **Utils Module** (`utils`): Performance monitoring and utility functions

## ğŸ“¦ å®‰è£…å’Œæ„å»º

### ç³»ç»Ÿè¦æ±‚

- Rust 1.70+
- å†…å­˜: å»ºè®®4GBä»¥ä¸Š
- å­˜å‚¨: çº¦2GBç”¨äºæ¨¡å‹æ–‡ä»¶
- ç½‘ç»œ: é¦–æ¬¡ä¸‹è½½æ¨¡å‹éœ€è¦ç½‘ç»œè¿æ¥

### ä»æºç æ„å»º

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/passchaos/cuttle.git
cd cuttle

# è°ƒè¯•æ„å»º
cargo build

# å‘å¸ƒæ„å»ºï¼ˆæ¨èç”¨äºå®é™…ä½¿ç”¨ï¼‰
cargo build --release

# å®‰è£…å‘½ä»¤è¡Œå·¥å…·
cargo install --path .
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä¸‹è½½Qwen3-0.6Bæ¨¡å‹

```bash
# ä¸‹è½½Qwen3-0.6Bæ¨¡å‹æ–‡ä»¶åˆ°assetsç›®å½•
cargo run -- download

# å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼ˆå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼‰
cargo run -- download --force
```

### 2. æ–‡æœ¬ç”Ÿæˆ

```bash
# ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆ
cargo run -- generate --prompt "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"

# è‹±æ–‡æ–‡æœ¬ç”Ÿæˆ
cargo run -- generate --prompt "Hello, how are you?"

# äº¤äº’å¼æ¨¡å¼
cargo run -- generate --interactive

# è‡ªå®šä¹‰å‚æ•°
cargo run -- generate \
  --prompt "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚" \
  --max-length 200 \
  --temperature 0.8 \
  --top-p 0.9
```

### 3. æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯

```bash
# æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
cargo run -- info
```

## ğŸ’» Programming Interface

### Basic Usage

```rust
use cuttle::{
    InferenceEngine, Model, ModelConfig, 
    Tokenizer, InferenceConfig
};

// Create model configuration
let config = ModelConfig::default();
let model = Model::new(config)?;

// Create tokenizer
let mut tokenizer = cuttle::tokenizer::create_default_tokenizer();
let texts = vec!["hello world".to_string()];
tokenizer.build_vocab(&texts)?;

// Create inference engine
let engine = InferenceEngine::new(model, tokenizer);

// Generate text
let response = engine.generate("Hello, how are you?")?;
println!("Generated: {}", response);
```

### Custom Inference Configuration

```rust
let inference_config = InferenceConfig {
    max_length: 512,
    temperature: 0.8,
    top_p: 0.9,
    top_k: 50,
    do_sample: true,
    repetition_penalty: 1.1,
};

let engine = InferenceEngine::with_config(model, tokenizer, inference_config);
```

### Batch Processing

```rust
let prompts = vec![
    "What is AI?".to_string(),
    "Explain machine learning".to_string(),
    "How does deep learning work?".to_string(),
];

let responses = engine.generate_batch(&prompts)?;
for (prompt, response) in prompts.iter().zip(responses.iter()) {
    println!("Q: {}\nA: {}\n", prompt, response);
}
```

### Tensor Operations

```rust
use cuttle::tensor::Tensor;

// Create tensors
let a = Tensor::randn(&[128, 256])?;
let b = Tensor::randn(&[256, 512])?;

// Matrix multiplication
let c = a.matmul(&b)?;

// Activation function
let activated = c.gelu();

// Softmax
let probs = activated.softmax(1)?;
```

## âš™ï¸ Configuration

### Model Configuration (config.json)

```json
{
  "vocab_size": 32000,
  "hidden_size": 4096,
  "num_layers": 32,
  "num_attention_heads": 32,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "rms_norm_eps": 1e-6
}
```

### é…ç½®é€‰é¡¹

- `--max-length`: æœ€å¤§ç”Ÿæˆé•¿åº¦ (é»˜è®¤: 512)
- `--temperature`: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ (é»˜è®¤: 1.0)
- `--top-p`: Top-pé‡‡æ ·å‚æ•° (é»˜è®¤: 0.9)
- `--top-k`: Top-ké‡‡æ ·å‚æ•° (é»˜è®¤: 50)
- `--interactive`: äº¤äº’å¼æ¨¡å¼
- `--force`: å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å‹

## ğŸ“Š Performance Benchmarks

Run benchmarks:

```bash
# Run all benchmarks
cargo bench

# Run specific benchmarks
cargo bench tensor_operations
cargo bench inference
```

### Performance Optimization Tips

1. **Compilation Optimization**: Use `--release` mode
2. **Pure Rust Implementation**: No external BLAS dependencies required
3. **Parallel Processing**: Utilize Rayon for parallel computation
4. **Memory Management**: Avoid unnecessary memory allocations

## ğŸ§ª Testing

```bash
# Run unit tests
cargo test

# Run integration tests
cargo test --test integration

# Run documentation tests
cargo test --doc
```

## ğŸ“š API Documentation

Generate and view API documentation:

```bash
cargo doc --open
```

## ğŸ› ï¸ Development

### é¡¹ç›®ç»“æ„

```
cuttle/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs          # åº“å…¥å£
â”‚   â”œâ”€â”€ main.rs         # å‘½ä»¤è¡Œå·¥å…·
â”‚   â”œâ”€â”€ model.rs        # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ inference.rs    # æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ tensor.rs       # å¼ é‡è¿ç®—
â”‚   â”œâ”€â”€ tokenizer.rs    # åˆ†è¯å™¨
â”‚   â”œâ”€â”€ downloader.rs   # æ¨¡å‹ä¸‹è½½å™¨
â”‚   â”œâ”€â”€ error.rs        # é”™è¯¯å¤„ç†
â”‚   â””â”€â”€ utils.rs        # å·¥å…·å‡½æ•°
â”œâ”€â”€ assets/             # æ¨¡å‹æ–‡ä»¶å­˜å‚¨ç›®å½•
â”‚   â””â”€â”€ qwen3-0.6b/    # Qwen3-0.6Bæ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ examples/           # ç¤ºä¾‹ä»£ç 
â”œâ”€â”€ benches/           # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ tests/             # é›†æˆæµ‹è¯•
â”œâ”€â”€ Cargo.toml         # é¡¹ç›®é…ç½®
â””â”€â”€ README.md          # é¡¹ç›®æ–‡æ¡£
```

## ğŸ¤– Qwen3-0.6Bæ¨¡å‹é…ç½®

- **å‚æ•°é‡**: 0.6B
- **è¯æ±‡è¡¨å¤§å°**: 151,936
- **éšè—å±‚ç»´åº¦**: 1,024
- **å±‚æ•°**: 28
- **æ³¨æ„åŠ›å¤´æ•°**: 16
- **é”®å€¼å¤´æ•°**: 8 (GQA)
- **æ”¯æŒè¯­è¨€**: ä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šè¯­è¨€

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆ

```bash
cargo run -- generate --prompt "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚" --max-length 150
```

### è‹±æ–‡æ–‡æœ¬ç”Ÿæˆ

```bash
cargo run -- generate --prompt "Explain quantum computing in simple terms." --max-length 200
```

### äº¤äº’å¼å¯¹è¯

```bash
cargo run -- generate --interactive
```

### Contributing Guidelines

1. Fork the project
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Create a Pull Request

### Code Style

- Use `rustfmt` to format code
- Use `clippy` for code linting
- Write comprehensive documentation and tests

```bash
# Format code
cargo fmt

# Code linting
cargo clippy
```

## ğŸ”§ Troubleshooting

### Common Issues

**Q: Compilation errors**

A: Ensure you have the latest Rust toolchain:

```bash
# Update Rust
rustup update

# Use Rust 2024 edition
rustup toolchain install nightly
```

**Q: Slow inference speed**

A: Check the following optimization options:
- Compile with `--release` mode
- Adjust batch processing size
- Use smaller models for testing
- Enable parallel processing

**Q: High memory usage**

A: Try the following approaches:
- Reduce model size
- Lower batch processing size
- Use smaller sequence lengths

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [rayon](https://github.com/rayon-rs/rayon) - Parallel computing framework
- [serde](https://github.com/serde-rs/serde) - Serialization framework
- [clap](https://github.com/clap-rs/clap) - Command line argument parsing
- [tokio](https://github.com/tokio-rs/tokio) - Asynchronous runtime

## ğŸ”— Related Links

- [Documentation](https://docs.rs/cuttle)
- [Examples](./examples)
- [Changelog](./CHANGELOG.md)
- [Contributing Guide](./CONTRIBUTING.md)

---

**Cuttle** - Power your AI inference with Rust ğŸ¦€âœ¨
